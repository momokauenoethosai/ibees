#!/usr/bin/env python3
"""
å…¨å±¥æ­´ç”»åƒé€ä¿¡ã«ã‚ˆã‚‹åå¾©èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
åå¾©Nå›ç›®ã§ã¯ã€å…ƒç”»åƒ + åå¾©1~N-1ã®å…¨ç”»åƒ + æœ€æ–°ç”»åƒã‚’Geminiã«é€ä¿¡
"""

import json
import sys
import time
from pathlib import Path
from PIL import Image
import google.generativeai as genai

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))
from face_composer.face_composer import FaceComposer

# Geminiè¨­å®š
GEMINI_API_KEY = "AIzaSyAt-wzZ3WLU1fc6fnzHvDhPsTZJNKnHszU"
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash-exp')

# ç›¸å¯¾èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—
ADJUSTMENT_STEPS = {
    'position': {
        'up': (0, -5), 'down': (0, 5), 'left': (-5, 0), 'right': (5, 0),
        'up_slight': (0, -3), 'down_slight': (0, 3), 'left_slight': (-3, 0), 'right_slight': (3, 0)
    },
    'scale': {
        'bigger': 0.05, 'smaller': -0.05, 'bigger_slight': 0.03, 'smaller_slight': -0.03
    }
}

def create_progressive_analysis_prompt(parts_list: list, iteration: int, adjustment_history: list, total_images: int) -> str:
    """å…¨å±¥æ­´åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    parts_str = ", ".join(parts_list)
    
    # ç”»åƒã®é †åºèª¬æ˜
    image_description = f"""
## ğŸ“· ç”»åƒã®é †åºï¼ˆè¨ˆ{total_images}æšï¼‰
**ç”»åƒ1**: å…ƒã®å®Ÿéš›ã®é¡”å†™çœŸï¼ˆæœ€çµ‚ç›®æ¨™ï¼‰
"""
    
    if total_images > 2:
        for i in range(2, total_images):
            image_description += f"**ç”»åƒ{i}**: åå¾©{i-1}ã®ä¼¼é¡”çµµï¼ˆå¤‰åŒ–éç¨‹ï¼‰\n"
    
    image_description += f"**ç”»åƒ{total_images}**: æœ€æ–°ã®ä¼¼é¡”çµµï¼ˆèª¿æ•´å¯¾è±¡ï¼‰"
    
    # èª¿æ•´å±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆåŒ–
    history_text = ""
    if adjustment_history:
        history_text = f"\n## ğŸ“ˆ å¤‰åŒ–å±¥æ­´ï¼ˆ{len(adjustment_history)}å›ã®èª¿æ•´ï¼‰\n"
        for i, hist in enumerate(adjustment_history, 1):
            similarity_before = hist.get('similarity_before', 0.0)
            similarity_after = hist.get('similarity_after', 0.0)
            adjustments = hist.get('adjustments', {})
            
            history_text += f"**åå¾©{i}**: "
            if adjustments:
                adj_summary = ", ".join([f"{part}({adj.get('position', '')}{adj.get('scale', '')})" 
                                       for part, adj in adjustments.items()])
                history_text += f"{adj_summary}\n"
                history_text += f"  â†’ é¡ä¼¼åº¦: {similarity_before:.2f} â†’ {similarity_after:.2f} (å¤‰åŒ–{similarity_after-similarity_before:+.2f})\n"
            else:
                history_text += "èª¿æ•´ãªã—\n"
            history_text += "\n"
    
    return f"""
ğŸ¯ å…¨å±¥æ­´é€²åŒ–åˆ†æ - åå¾©{iteration}å›ç›®

{image_description}

## ğŸ” åˆ†æç›®æ¨™
**é€²åŒ–ã®æµã‚Œã‚’æŠŠæ¡ã—ã€ç”»åƒ{total_images}ï¼ˆæœ€æ–°ï¼‰ã‚’ç”»åƒ1ï¼ˆå…ƒå†™çœŸï¼‰ã«ã‚ˆã‚Šè¿‘ã¥ã‘ã‚‹**

å…¨ã¦ã®ç”»åƒã‚’è¦‹ã‚‹ã“ã¨ã§ï¼š
1. **å¤‰åŒ–ã®æ–¹å‘æ€§**: ã©ã®èª¿æ•´ãŒæ”¹å–„/æ‚ªåŒ–ã‚’ã‚‚ãŸã‚‰ã—ãŸã‹
2. **åæŸãƒ‘ã‚¿ãƒ¼ãƒ³**: ç›®æ¨™ã«è¿‘ã¥ã„ã¦ã„ã‚‹ã‹ã€è¿·èµ°ã—ã¦ã„ã‚‹ã‹  
3. **æ¬¡ã®æœ€é©æ‰‹**: éå»ã®æˆåŠŸ/å¤±æ•—ã‚’è¸ã¾ãˆãŸæœ€è‰¯ã®æ¬¡æ‰‹

{history_text}

## ğŸš¨ é‡è¦ï¼šäººé–“ã®æ„Ÿè¦šã«ã‚ˆã‚‹å³æ ¼è©•ä¾¡

### è§£å‰–å­¦çš„ç•°å¸¸ã®å¼·åˆ¶æ¤œå‡º
ç”»åƒ{total_images}ã§ä»¥ä¸‹ãŒç¢ºèªã•ã‚ŒãŸã‚‰ **æœ€å„ªå…ˆä¿®æ­£**:
1. **ç›®ã®é–“éš”ç•°å¸¸**: 40pxæœªæº€ï¼ˆç‹­ã™ãï¼‰ã¾ãŸã¯ 70pxè¶…ï¼ˆé›¢ã‚Œã™ãï¼‰
2. **å£ã‚µã‚¤ã‚ºç•°å¸¸**: é¡”å¹…ã®15%æœªæº€ï¼ˆå°ã•ã™ãï¼‰ã¾ãŸã¯ 40%è¶…ï¼ˆå¤§ãã™ãï¼‰  
3. **çœ‰-ç›®è·é›¢ç•°å¸¸**: 10pxæœªæº€ï¼ˆè¿‘ã™ãï¼‰ã¾ãŸã¯ 40pxè¶…ï¼ˆé›¢ã‚Œã™ãï¼‰
4. **å…¨ä½“ãƒãƒ©ãƒ³ã‚¹å´©å£Š**: äººé–“ãŒè¦‹ã¦æ˜ã‚‰ã‹ã«ä¸è‡ªç„¶

### ğŸ“Š è©•ä¾¡åŸºæº–
- **human_perception_score**: äººé–“ãŒè¦‹ãŸè‡ªç„¶ã•ï¼ˆ0.0-1.0ï¼‰
- **similarity_score**: å…ƒç”»åƒã¨ã®é¡ä¼¼åº¦ï¼ˆ0.0-1.0ï¼‰
- **progression_score**: éå»ã‹ã‚‰ã®æ”¹å–„åº¦ï¼ˆ-1.0 to +1.0ï¼‰

## ğŸ“‹ å¯¾è±¡ãƒ‘ãƒ¼ãƒ„: {parts_str}

## âš™ï¸ èª¿æ•´æˆ¦ç•¥

### ğŸ¯ å±¥æ­´æ´»ç”¨ã«ã‚ˆã‚‹åˆ¤æ–­
- **æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ç¶™ç¶š**: é¡ä¼¼åº¦ãŒå‘ä¸Šã—ãŸèª¿æ•´ã¯åŒæ–¹å‘ã§ç¶™ç¶š
- **å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³å›é¿**: é¡ä¼¼åº¦ãŒæ‚ªåŒ–ã—ãŸèª¿æ•´ã¯é€†æ–¹å‘ã«ä¿®æ­£
- **è¿·èµ°ãƒ‘ã‚¿ãƒ¼ãƒ³è„±å‡º**: æŒ¯å‹•ã—ã¦ã„ã‚‹å ´åˆã¯åˆ¥ã®ãƒ‘ãƒ¼ãƒ„ã«æ³¨ç›®

### ğŸš¨ å¼·åˆ¶ä¿®æ­£ãƒ¢ãƒ¼ãƒ‰
äººé–“æ„Ÿè¦šã‚¹ã‚³ã‚¢ < 0.4 ã®å ´åˆï¼š
- è§£å‰–å­¦çš„ç•°å¸¸ã‚’æœ€å„ªå…ˆä¿®æ­£
- é¡ä¼¼åº¦ã‚ˆã‚Šè‡ªç„¶ã•ã‚’é‡è¦–
- ã‚ˆã‚Šå¤§ããªèª¿æ•´ï¼ˆup/down/bigger/smallerï¼‰ã‚’æ¡ç”¨

## å‡ºåŠ›å½¢å¼
```json
{{
  "progression_analysis": {{
    "improvement_trend": "improving|stagnating|declining",
    "successful_adjustments": ["outline scale bigger", "mouth position up"],
    "failed_adjustments": ["eye position left"],
    "next_strategy": "continue_successful|try_different|force_correction"
  }},
  "debug_analysis": {{
    "human_perception_score": 0.3,
    "anomalies_detected": [
      "ç›®ã®é–“éš”ãŒç•°å¸¸ã«åºƒã„ï¼ˆç´„75pxï¼‰",
      "å£ãŒé¡”å…¨ä½“ã«å¯¾ã—ã¦ä¸é‡£ã‚Šåˆã„ã«å¤§ãã„"
    ]
  }},
  "comparison_analysis": {{
    "similarity_score": 0.4,
    "relationship_differences": [
      "ç”»åƒ1ã¨æ¯”è¼ƒã—ã¦å…¨ä½“çš„ãªãƒ‘ãƒ¼ãƒ„ãƒãƒ©ãƒ³ã‚¹ãŒä¸è‡ªç„¶"
    ]
  }},
  "adjustments": {{
    "eye": {{"position": "right", "reason": "ç•°å¸¸ã«é›¢ã‚ŒãŸç›®ã‚’è‡ªç„¶ãªé–“éš”ã«å¼·åˆ¶ä¿®æ­£"}},
    "mouth": {{"scale": "smaller", "reason": "ç•°å¸¸ã«å¤§ãã„å£ã‚’äººé–“ã®æ„Ÿè¦šã«åˆã‚ã›ã¦ä¿®æ­£"}}
  }},
  "satisfied": false,
  "notes": "äººé–“ã®æ„Ÿè¦šã§ä¸è‡ªç„¶ãªéƒ¨åˆ†ã‚’è§£å‰–å­¦çš„åŸºæº–ã§å¼·åˆ¶ä¿®æ­£"
}}
```

**æœ€é‡è¦**: å…¨ã¦ã®å±¥æ­´ç”»åƒã®å¤‰åŒ–ã‚’è¦‹ã¦ã€äººé–“ãŒè‡ªç„¶ã¨æ„Ÿã˜ã‚‹æ–¹å‘ã«å°ã„ã¦ãã ã•ã„ã€‚
    """

def load_parts_from_json(json_path: str) -> dict:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ¼ãƒ„æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
    
    def find_part_image_path(category: str, part_num: int) -> Path:
        assets_root = Path("kawakura/assets_png")
        category_mapping = {
            'mouth': 'mouse', 'hair': 'hair', 'eye': 'eye', 'eyebrow': 'eyebrow',
            'nose': 'nose', 'ear': 'ear', 'outline': 'outline', 'acc': 'acc',
            'beard': 'beard', 'glasses': 'glasses', 'extras': 'extras'
        }
        
        folder_name = category_mapping.get(category, category)
        category_folder = assets_root / folder_name
        file_prefix = folder_name
        
        candidates = [
            f"{file_prefix}_{part_num:03d}.png",
            f"{file_prefix}_{part_num:02d}.png", 
            f"{file_prefix}_{part_num}.png"
        ]
        
        for candidate in candidates:
            part_path = category_folder / candidate
            if part_path.exists():
                return part_path
        return None
    
    with open(json_path, 'r', encoding='utf-8') as f:
        analysis_result = json.load(f)
    
    parts_dict = {}
    parts = analysis_result.get('parts', {})
    
    for category, part_info in parts.items():
        selected = part_info.get('selected', {})
        part_num = selected.get('part_num')
        score = selected.get('score', 0.0)
        
        if part_num:
            part_image_path = find_part_image_path(category, part_num)
            if part_image_path:
                parts_dict[category] = {
                    'part_id': f"{category}_{part_num:03d}",
                    'image_path': part_image_path,
                    'part_num': part_num,
                    'score': score
                }
    
    return parts_dict

def get_original_image_path(json_path: str) -> Path:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…ƒç”»åƒã®ãƒ‘ã‚¹ã‚’å–å¾—"""
    with open(json_path, 'r', encoding='utf-8') as f:
        analysis_result = json.load(f)
    
    input_image = analysis_result.get('input_image', '')
    
    if input_image.startswith('/Users/'):
        original_path = Path(input_image)
    else:
        original_path = Path(input_image)
    
    if not original_path.exists():
        filename = Path(input_image).name
        for search_dir in ["uploads", "made_pictures"]:
            candidate = Path(search_dir) / filename
            if candidate.exists():
                return candidate
    
    return original_path if original_path.exists() else None

def apply_relative_adjustments(current_positions: dict, adjustments: dict) -> dict:
    """ç›¸å¯¾èª¿æ•´ã‚’åº§æ¨™ã«é©ç”¨"""
    new_positions = json.loads(json.dumps(current_positions))
    
    for category, adj_info in adjustments.items():
        if category not in new_positions:
            continue
            
        pos_adj = adj_info.get('position')
        scale_adj = adj_info.get('scale')
        reason = adj_info.get('reason', '')
        current_pos = new_positions[category]
        
        if isinstance(current_pos, dict):
            # å·¦å³å¯¾ç§°ãƒ‘ãƒ¼ãƒ„
            for side in ['left', 'right']:
                if side in current_pos and len(current_pos[side]) >= 3:
                    x, y, scale = current_pos[side]
                    
                    if pos_adj and pos_adj in ADJUSTMENT_STEPS['position']:
                        dx, dy = ADJUSTMENT_STEPS['position'][pos_adj]
                        x, y = x + dx, y + dy
                    
                    if scale_adj and scale_adj in ADJUSTMENT_STEPS['scale']:
                        scale_delta = ADJUSTMENT_STEPS['scale'][scale_adj]
                        scale = max(0.1, min(1.0, scale + scale_delta))
                    
                    new_positions[category][side] = (x, y, scale)
            
            print(f"  [ADJUST] {category}: {adj_info} - {reason}")
        else:
            # å˜ä¸€ãƒ‘ãƒ¼ãƒ„
            if len(current_pos) >= 3:
                x, y, scale = current_pos
                
                if pos_adj and pos_adj in ADJUSTMENT_STEPS['position']:
                    dx, dy = ADJUSTMENT_STEPS['position'][pos_adj]
                    x, y = x + dx, y + dy
                
                if scale_adj and scale_adj in ADJUSTMENT_STEPS['scale']:
                    scale_delta = ADJUSTMENT_STEPS['scale'][scale_adj]
                    scale = max(0.1, min(1.0, scale + scale_delta))
                
                new_positions[category] = (x, y, scale)
                
            print(f"  [ADJUST] {category}: {adj_info} â†’ ({x}, {y}, {scale:.2f}) - {reason}")
    
    return new_positions

def save_debug_session(session_id: str, iteration: int, prompt: str, input_images: list, response_text: str, parsed_result: dict = None):
    """ãƒ‡ãƒãƒƒã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿å­˜"""
    debug_dir = Path("outputs/debug_sessions") / session_id
    debug_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿å­˜
    prompt_file = debug_dir / f"iteration_{iteration}_prompt.txt"
    prompt_file.write_text(prompt, encoding='utf-8')
    
    # é€ä¿¡ç”»åƒä¿å­˜
    for i, img in enumerate(input_images):
        if isinstance(img, Image.Image):
            img_file = debug_dir / f"iteration_{iteration}_input_image_{i+1}.png"
            img.save(img_file)
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¿å­˜
    response_file = debug_dir / f"iteration_{iteration}_response.txt"
    response_file.write_text(response_text, encoding='utf-8')
    
    # è§£æçµæœä¿å­˜
    if parsed_result:
        result_file = debug_dir / f"iteration_{iteration}_parsed.json"
        result_file.write_text(json.dumps(parsed_result, ensure_ascii=False, indent=2), encoding='utf-8')
    
    return debug_dir

def progressive_history_test(json_path: str, max_iterations: int = 5):
    """å…¨å±¥æ­´é€ä¿¡ã«ã‚ˆã‚‹åå¾©èª¿æ•´ãƒ†ã‚¹ãƒˆ"""
    
    session_id = f"progressive_{time.strftime('%Y%m%d_%H%M%S')}"
    print(f"ğŸ“ˆ å…¨å±¥æ­´é€²åŒ–åˆ†æãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"ğŸ†” ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {session_id}")
    print(f"ğŸ“„ JSONãƒ•ã‚¡ã‚¤ãƒ«: {json_path}")
    
    # 1. å…ƒç”»åƒã¨ãƒ‘ãƒ¼ãƒ„æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    try:
        original_image_path = get_original_image_path(json_path)
        if not original_image_path or not original_image_path.exists():
            print(f"âŒ å…ƒç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {original_image_path}")
            return
        
        original_image = Image.open(original_image_path).resize((400, 400), Image.LANCZOS)
        print(f"ğŸ“¸ å…ƒç”»åƒ: {original_image_path}")
        
        parts_dict = load_parts_from_json(json_path)
        print(f"âœ… {len(parts_dict)}å€‹ã®ãƒ‘ãƒ¼ãƒ„èª­ã¿è¾¼ã¿: {list(parts_dict.keys())}")
        
    except Exception as e:
        print(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    # 2. åˆæœŸåº§æ¨™
    current_positions = {
        'hair': (200, 200, 1.0),
        'eye': {'left': (225, 215, 0.2), 'right': (175, 215, 0.2)},
        'eyebrow': {'left': (225, 185, 0.2), 'right': (175, 185, 0.2)},
        'nose': (200, 230, 0.2),
        'mouth': (200, 255, 0.25),
        'ear': {'left': (250, 220, 0.28), 'right': (150, 220, 0.28)},
        'outline': (200, 200, 1.0),
        'acc': (200, 180, 0.3),
        'beard': (200, 300, 0.4),
        'glasses': (200, 215, 0.5)
    }
    
    iteration_images = []
    adjustment_history = []
    composer = FaceComposer(canvas_size=(400, 400))
    start_time = time.time()
    
    # 3. åå¾©ãƒ«ãƒ¼ãƒ—
    for iteration in range(max_iterations):
        print(f"\n--- ğŸ“ˆ åå¾© {iteration + 1}/{max_iterations} ï¼ˆå…¨å±¥æ­´åˆ†æï¼‰---")
        
        # 3.1 ç¾åœ¨åº§æ¨™ã§åˆæˆ
        print("ğŸ¨ åˆæˆä¸­...")
        try:
            composed_image = composer.compose_face_with_custom_positions(
                base_image_path=None,
                parts_dict=parts_dict,
                custom_positions=current_positions
            )
            
            if not composed_image:
                print(f"âŒ åå¾©{iteration + 1}: åˆæˆå¤±æ•—")
                break
            
            # RGBå¤‰æ›ã¨ãƒªã‚µã‚¤ã‚º
            if composed_image.mode == 'RGBA':
                background = Image.new('RGB', composed_image.size, (255, 255, 255))
                background.paste(composed_image, mask=composed_image.split()[-1])
                composed_image = background
            
            current_image = composed_image.resize((400, 400), Image.LANCZOS)
            
            # ç”»åƒä¿å­˜
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            iteration_filename = f"progressive_{iteration + 1}_{timestamp}.png"
            iteration_path = Path("outputs") / iteration_filename
            current_image.save(iteration_path)
            iteration_images.append(current_image)  # PIL Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜
            print(f"ğŸ’¾ åå¾©ç”»åƒ: {iteration_path}")
            
        except Exception as e:
            print(f"âŒ åˆæˆã‚¨ãƒ©ãƒ¼: {e}")
            break
        
        # 3.2 å…¨å±¥æ­´ã§Geminiåˆ†æ
        total_images = len(iteration_images) + 1  # å…ƒç”»åƒ + å…¨åå¾©ç”»åƒ
        print(f"ğŸ¤–ğŸ“ˆ Gemini å…¨å±¥æ­´åˆ†æä¸­ï¼ˆ{total_images}ç”»åƒï¼‰...")
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = create_progressive_analysis_prompt(
                list(parts_dict.keys()),
                iteration + 1, 
                adjustment_history,
                total_images
            )
            
            # å…¨ç”»åƒã‚’é€ä¿¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
            gemini_input = [prompt, original_image] + iteration_images
            
            print(f"ğŸ“· é€ä¿¡: å…ƒç”»åƒ + åå¾©1~{iteration+1} = è¨ˆ{total_images}ç”»åƒ")
            print(f"ğŸ“Š å¤‰åŒ–è¿½è·¡: å…¨ã¦ã®é€²åŒ–éç¨‹ã‚’GeminiãŒæŠŠæ¡")
            
            # Geminiå‘¼ã³å‡ºã—
            response = model.generate_content(gemini_input)
            
            if not response.text:
                print(f"âŒ åå¾©{iteration + 1}: Geminiå¿œç­”ãªã—")
                break
            
            response_text = response.text
            print(f"ğŸ“‹ Geminiå¿œç­”: {len(response_text)}æ–‡å­—")
            print(f"ğŸ“ ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {response_text[:200]}...")
            
            # JSONè§£æ
            try:
                response_clean = response_text.strip()
                if '```json' in response_clean:
                    start_idx = response_clean.find('```json') + 7
                    end_idx = response_clean.find('```', start_idx)
                    code_block = response_clean[start_idx:end_idx].strip()
                else:
                    code_block = response_clean
                
                adjustment_result = json.loads(code_block)
                
                # çµæœç¢ºèª
                progression = adjustment_result.get('progression_analysis', {})
                debug_analysis = adjustment_result.get('debug_analysis', {})
                comparison = adjustment_result.get('comparison_analysis', {})
                
                human_score = debug_analysis.get('human_perception_score', 0.0)
                similarity_score = comparison.get('similarity_score', 0.0)
                trend = progression.get('improvement_trend', 'unknown')
                anomalies = debug_analysis.get('anomalies_detected', [])
                
                satisfied = adjustment_result.get('satisfied', False)
                adjustments = adjustment_result.get('adjustments', {})
                notes = adjustment_result.get('notes', '')
                
                print(f"\nğŸ“Š åˆ†æçµæœ:")
                print(f"  ğŸ­ äººé–“æ„Ÿè¦šã‚¹ã‚³ã‚¢: {human_score:.2f}")
                print(f"  ğŸ¯ é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {similarity_score:.2f}")
                print(f"  ğŸ“ˆ æ”¹å–„å‚¾å‘: {trend}")
                print(f"  ğŸš¨ ç•°å¸¸æ¤œå‡º: {anomalies}")
                print(f"  ğŸ˜Š æº€è¶³åº¦: {satisfied}")
                print(f"  ğŸ”§ èª¿æ•´æŒ‡ç¤º: {adjustments}")
                print(f"  ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆ: {notes}")
                
                # ãƒ‡ãƒãƒƒã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
                debug_dir = save_debug_session(
                    session_id, iteration + 1, prompt, 
                    [original_image] + iteration_images, 
                    response_text, adjustment_result
                )
                
                # å±¥æ­´è¨˜éŒ²
                history_entry = {
                    'iteration': iteration + 1,
                    'similarity_before': 0.0 if iteration == 0 else adjustment_history[-1].get('similarity_after', 0.0),
                    'similarity_after': similarity_score,
                    'human_perception_score': human_score,
                    'improvement_trend': trend,
                    'adjustments': adjustments,
                    'notes': notes,
                    'anomalies': anomalies
                }
                adjustment_history.append(history_entry)
                
                # æº€è¶³ã¾ãŸã¯èª¿æ•´ãªã—ãªã‚‰çµ‚äº†
                if satisfied or not adjustments:
                    print(f"âœ… åå¾©{iteration + 1}: ç›®æ¨™é”æˆï¼ï¼ˆäººé–“æ„Ÿè¦š: {human_score:.2f}, é¡ä¼¼åº¦: {similarity_score:.2f}ï¼‰")
                    break
                
                # ç›¸å¯¾èª¿æ•´ã‚’é©ç”¨
                print("âš™ï¸ å±¥æ­´å­¦ç¿’ã«ã‚ˆã‚‹èª¿æ•´ã‚’é©ç”¨...")
                current_positions = apply_relative_adjustments(current_positions, adjustments)
                
            except json.JSONDecodeError as e:
                error_msg = f"JSONè§£æå¤±æ•—: {e}"
                print(f"âŒ {error_msg}")
                print(f"ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_text}")
                
                # ã‚¨ãƒ©ãƒ¼ã‚‚ãƒ‡ãƒãƒƒã‚°ä¿å­˜
                save_debug_session(
                    session_id, iteration + 1, prompt,
                    [original_image] + iteration_images,
                    response_text
                )
                break
                
        except Exception as e:
            print(f"âŒ Geminiå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            break
    
    # 4. çµæœãƒ¬ãƒãƒ¼ãƒˆ
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\nğŸ å…¨å±¥æ­´é€²åŒ–åˆ†æå®Œäº†")
    print(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’")
    print(f"ğŸ–¼ï¸ ç”Ÿæˆç”»åƒæ•°: {len(iteration_images)}æš")
    print(f"ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«: outputs/debug_sessions/{session_id}/")
    
    if adjustment_history:
        print(f"\nğŸ“ˆ é€²åŒ–å±¥æ­´:")
        for hist in adjustment_history:
            human_score = hist.get('human_perception_score', 0.0)
            similarity_score = hist.get('similarity_after', 0.0)
            trend = hist.get('improvement_trend', 'unknown')
            print(f"  åå¾©{hist['iteration']}: äººé–“{human_score:.2f}, é¡ä¼¼{similarity_score:.2f}, å‚¾å‘{trend}")

def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ³•:")
        print("  python progressive_history_refiner.py <json_path> [max_iterations]")
        print("\nä¾‹:")
        print("  python progressive_history_refiner.py outputs/run_20250830_170700.json")
        print("  python progressive_history_refiner.py outputs/run_20250830_170700.json 3")
        return
    
    json_path = sys.argv[1]
    max_iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    progressive_history_test(json_path, max_iterations)

if __name__ == "__main__":
    main()